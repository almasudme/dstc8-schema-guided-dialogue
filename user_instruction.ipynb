{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install python-dotenv\n",
    "# %pip install pandas\n",
    "# %pip install --upgrade langchain\n",
    "# %pip install langchain-community langchain-core\n",
    "# %pip install -q -U google-generativeai\n",
    "# %pip install -U langchain-openai\n",
    "# %pip install -q -U google-generativeai\n",
    "# %pip install -qU langchain-google-genai\n",
    "# %pip install -U langchain-ollama\n",
    "# # !ollama pull llama3.1\n",
    "# %pip install -qU langchain_mistralai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "from  openai  import OpenAI\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv())\n",
    "# print(os.environ[\"GOOGLE_API_KEY\"])\n",
    "# print(os.environ[\"LANGCHAIN_API_KEY\"])\n",
    "# print(os.environ[\"OPENAI_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER:Hi i need a help, i am very hungry, I am looking for a restaurant\n",
      "SYSTEM:Sure, I will help you, What type of food are you looking for? Which city should i search in?\n",
      "USER:Some Punjabi kind of foods in milpitas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file_name = \"train_mini.csv\"\n",
    "# file_name = \"train_full.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_file_name)\n",
    "df.shape\n",
    "conversation = df.iloc[1]['conversation']\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Variable declarations and prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a user asking a system to {intent}. Your role is to create a one line user instruction. You have to read the conversation based on this intention as the person from the conversation.\n",
    "\n",
    "Focus on the requirement, whether user is asking you to {intent} the name of the place,time and party size. \n",
    "Also identify required number of people, time , place or any other available entities and features to {intent} in the conversation.\n",
    "Do not display these entities separately though.\n",
    "Use this entities in various order to construct the user instruction. \n",
    "\n",
    "\n",
    "\n",
    "Do not make up information if it is not availble.\n",
    "Keep the same verb from conversation in the instruction that is used to {intent}.\n",
    "Use diverse mood and personality in the instruction. \n",
    "Do not start with Hey.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "intent_dict = {\n",
    "    'ReserveRestaurant' : 'Reserve a Restaurant',\n",
    "    'FindRestaurants':'Find a Restaurant',\n",
    "    'SearchHotel': 'Search a Hotel',\n",
    "    'ReserveHotel':'Reserve a Hotel'\n",
    "}\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",f\"{prompt_template}\",\n",
    "        ),\n",
    "        (\"human\", \"{conversation}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define LLM models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\flask_projects\\miniconda\\envs\\langchain_dev\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Define LLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gemini_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "openai_llm = ChatOpenAI(\n",
    "    model_name= \"gpt-4o-mini\", \n",
    "    temperature=0.5\n",
    "    )\n",
    "\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "ollama_llm = ChatOllama(\n",
    "    model = \"llama3.1\",\n",
    "    temperature = 0.3,\n",
    "    num_predict = 256,\n",
    "    # other params ...\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "mistral_llm = ChatMistralAI(\n",
    "    model=\"open-mistral-7b\",\n",
    "    temperature=0.5,\n",
    "    # max_retries=2,\n",
    "    # other params...\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "openai_chain = prompt | openai_llm\n",
    "gemini_chain = prompt | gemini_llm\n",
    "ollama_chain = prompt | ollama_llm\n",
    "mistral_chain = prompt | mistral_llm\n",
    "model_chains = {\n",
    "    # 'gpt': openai_chain,\n",
    "    'gemini':gemini_chain,\n",
    "    # 'mistral':mistral_chain,\n",
    "    # 'ollama':ollama_chain\n",
    "    }\n",
    "\n",
    "\n",
    "def execute_chain(chain,dialogue_id,intent,conversation):            \n",
    "\n",
    "    # Apply LLM chain on the desired column in each row\n",
    "    result = chain.invoke(\n",
    "        {\n",
    "        \"conversation\":dialogue_id +': '+ conversation,\n",
    "        \"intent\":intent\n",
    "        }\n",
    "    )\n",
    "    return result.content\n",
    "\n",
    "execute_following = False\n",
    "experiment = True\n",
    "if execute_following:\n",
    "    for model in model_chains.values():\n",
    "        chain = model_chains[model]\n",
    "        results = []\n",
    "        # # Experimentation : Iterate over each row\n",
    "        if experiment:\n",
    "            for i in  range(1,205,20):\n",
    "                    dialogue_id = df.iloc[i]['dialogue_id']\n",
    "                    conversation = df.iloc[i]['conversation']\n",
    "                    intent = intent_dict[df.iloc[i]['service_call_method']]\n",
    "                    if  intent != 'Reserve a Restaurant':continue\n",
    "                    result = execute_chain(chain,dialogue_id,intent,conversation)\n",
    "                    results.append(result)\n",
    "                    time.sleep(2)\n",
    "\n",
    "        else:   \n",
    "\n",
    "            # # Iterate over each row\n",
    "            for index, row in df.iterrows():\n",
    "                # Apply LLM chain on the desired column in each row\n",
    "                dialogue_id = row['dialogue_id']\n",
    "                intent = intent_dict[row['service_call_method']]\n",
    "                conversation = row['conversation']\n",
    "                result = execute_chain(chain,dialogue_id,intent,conversation)\n",
    "                results.append(result)\n",
    "                break\n",
    "                \n",
    "        print(f\"{model}: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2041513456.py, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 26\u001b[1;36m\u001b[0m\n\u001b[1;33m    if  not 'Reserve' in intent != :continue\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Update Dataframe /CSV\n",
    "header_list = ['dialogue_id', 'conversation', 'turn_services', 'intent','service_call_method', 'service_call_parameters', 'service_results']\n",
    "row_count = df.shape[0]\n",
    "\n",
    "for model in model_chains:\n",
    "    chain = model_chains[model]\n",
    "    llm_model_name = type(chain.steps[1]).__name__.replace('Chat','')\n",
    "    if llm_model_name not in header_list:\n",
    "        header_list.append(llm_model_name)\n",
    "        df = df.reindex(columns = header_list)    \n",
    "    if llm_model_name == 'MistralAI': time.sleep(10)\n",
    "    column_name = str(llm_model_name) + '_instruction'\n",
    "    \n",
    "    results =[]\n",
    "    for i in  range(0,row_count):\n",
    "        # if i > 100:\n",
    "        #     break\n",
    "        if not  pd.isna(df.iloc[i][llm_model_name]):\n",
    "            print(f\"Skipping generating instruction.Already available row {i} for {llm_model_name}: {df.iloc[i][llm_model_name]}\")\n",
    "            \n",
    "            continue\n",
    "\n",
    "        dialogue_id = df.iloc[i]['dialogue_id']\n",
    "        conversation = df.iloc[i]['conversation']\n",
    "        intent = intent_dict[df.iloc[i]['service_call_method']]\n",
    "        if  not 'Reserve' in intent  :continue\n",
    "        result = execute_chain(chain,dialogue_id,intent,conversation)\n",
    "        results.append(result)\n",
    "        df.loc[i,llm_model_name]=result.rstrip()\n",
    "        print(i,column_name,result)\n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save CSV File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(csv_file_name)\n",
    "df.to_csv(csv_file_name, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'202409170616'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "time.strftime('%Y%m%d%H%M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a CSV Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_csv_agent(\n",
    "    openai_llm,\n",
    "    csv_file_name,\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "    allow_dangerous_code = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"how many rows are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm feeling hungry and would like to find a restaurant in San Jose that serves American cuisine.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai_llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_50564\\2469970913.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mopenai_chain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprompt\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mopenai_llm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mgemini_chain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprompt\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mgemini_llm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mollama_chain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprompt\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mollama_llm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'openai_llm' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "Your role is to identify name of a place type and type of service in that place from a user instruction.\n",
    "\n",
    "Example:\n",
    "\n",
    "I'm looking for a restaurant in Oakland that serves pizza.\n",
    "Here Place is \"Oakland\" type is \"restaurant\" and other parameter is \"pizza\".\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",f\"{prompt_template}\",\n",
    "        ),\n",
    "        (\"human\", \"{user_instruction}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "openai_chain = prompt | openai_llm\n",
    "gemini_chain = prompt | gemini_llm\n",
    "ollama_chain = prompt | ollama_llm\n",
    "mistral_chain = prompt | mistral_llm\n",
    "\n",
    "def execute_chain(chain,user_instruction):            \n",
    "\n",
    "    # Apply LLM chain on the desired column in each row\n",
    "    result = chain.invoke(\n",
    "        {\n",
    "        \"user_instruction\":user_instruction,\n",
    "        \n",
    "        }\n",
    "    )\n",
    "    output_lines = result.content.splitlines()\n",
    "    output_dict = {}\n",
    "\n",
    "    # Iterate over each line, split by the colon, and strip extra spaces\n",
    "    for line in output_lines:\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            output_dict[key.strip()] = value.strip().strip('\"')\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_50564\\1543248773.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msearch_criteria\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtarget_place\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mchain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopenai_chain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_mini.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'openai_chain' is not defined"
     ]
    }
   ],
   "source": [
    "search_criteria=None\n",
    "target_place = None\n",
    "chain = openai_chain\n",
    "\n",
    "df = pd.read_csv('train_mini.csv')\n",
    "\n",
    "for i,row in df.iterrows():\n",
    "    user_instruction = row[\"OpenAI\"]\n",
    "    print(user_instruction)\n",
    "    result = execute_chain(chain,user_instruction)\n",
    "    print(result)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.5 ('langchain_dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9d81f49121f61b45910aa34f46ac101ca6a9b89634130b07cc43b9b56cc97cd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
