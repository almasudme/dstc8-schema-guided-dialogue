{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install python-dotenv\n",
    "# !pip install --upgrade langchain\n",
    "# !pip install langchain-community langchain-core\n",
    "# !pip install -q -U google-generativeai\n",
    "# !pip install -U langchain-openai\n",
    "# !pip install -q -U google-generativeai\n",
    "# !pip install -qU langchain-google-genai\n",
    "# !pip install -U langchain-ollama\n",
    "# !ollama pull llama3.1\n",
    "# !pip install -qU langchain_mistralai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "from  openai  import OpenAI\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv())\n",
    "# print(os.environ[\"GOOGLE_API_KEY\"])\n",
    "# print(os.environ[\"LANGCHAIN_API_KEY\"])\n",
    "# print(os.environ[\"OPENAI_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER:Hi i need a help, i am very hungry, I am looking for a restaurant\n",
      "SYSTEM:Sure, I will help you, What type of food are you looking for? Which city should i search in?\n",
      "USER:Some Punjabi kind of foods in milpitas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_name = \"train_mini.csv\"\n",
    "# file_name = \"train_full.csv\"\n",
    "\n",
    "df = pd.read_csv(file_name)\n",
    "df.shape\n",
    "conversation = df.iloc[1]['conversation']\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Variable declarations and prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Your role is to ask an agent to {intent}. \n",
    "Construct a one line user instruction based on this intention as first person from the conversation.\n",
    "\n",
    "Focus on whether user is asking you to {intent} the name of the place,time and party size. \n",
    "Also identify required number of people, time , place or any other available entities and features to {intent} in the conversation.\n",
    "Do not display these entities separately though.\n",
    "Use this information in various order to construct the user instruction. Do not make up information if it is not availble.\n",
    "Keep the same verb from conversation in the instruction that is used to {intent}.\n",
    "Use diverse mood and personality in the instruction.\n",
    "\"\"\"\n",
    "\n",
    "intent_dict = {\n",
    "    'ReserveRestaurant' : 'Reserve a Restaurant',\n",
    "    'FindRestaurants':'Find a Restaurant',\n",
    "    'SearchHotel': 'Search a Hotel',\n",
    "    'ReserveHotel':'Reserve a Hotel'\n",
    "}\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",f\"{prompt_template}\",\n",
    "        ),\n",
    "        (\"human\", \"{conversation}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define LLM models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gemini_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "openai_llm = ChatOpenAI(\n",
    "    model_name= \"gpt-4o-mini\", \n",
    "    temperature=0.5\n",
    "    )\n",
    "\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "ollama_llm = ChatOllama(\n",
    "    model = \"llama3.1\",\n",
    "    temperature = 0.5,\n",
    "    num_predict = 256,\n",
    "    # other params ...\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "mistral_llm = ChatMistralAI(\n",
    "    model=\"open-mistral-7b\",\n",
    "    temperature=0.5,\n",
    "    # max_retries=2,\n",
    "    # other params...\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt: [\"I'd like to reserve a table for 2 at Bird Dog in Palo Alto for 11:30 am today, please!\", 'Please reserve a table for 2 at the Grand Harbor in Burlingame today at 12:45 pm.', 'Can you reserve a table for 4 at Hunan Chili in Mountain View for 6 in the evening?', \"I'd like to make a reservation for two people at Lake Chalet in Oakland for 10:30 this morning.\", 'Can you please reserve a table for 4 at Soo Yuan Restaurant in Calistoga at 1:30 this afternoon?', \"I'd like to reserve a table for three at Chaat Corner in San Francisco on March 14th at 7:30 pm, please.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini: ['Can you please make a booking at this restaurant for me at 11:30 am? \\n', 'Please make a reservation at the Grand Harbor in Burlingame for quarter to 1 in the afternoon today. \\n', 'I want to reserve a table for 4 at 6 in the evening at Hunan Chili in Mountain View.  \\n', 'Can you please make a reservation for two people at Lake Chalet in Oakland at 10:30 in the morning? \\n', 'Please make reservation for 4 including me at Soo Yuan Restaurant in Calistoga at 1:30 pm today. \\n', 'Can you make a reservation for me at Chaat Corner in San Francisco for three people on March 14th at 7:30 pm? \\n']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "openai_chain = prompt | openai_llm\n",
    "gemini_chain = prompt | gemini_llm\n",
    "ollama_chain = prompt | ollama_llm\n",
    "mistral_chain = prompt | mistral_llm\n",
    "model_chains = {\n",
    "    'gpt': openai_chain,\n",
    "    'gemini':gemini_chain,\n",
    "    # 'mistral':mistral_chain,\n",
    "    # 'ollama':ollama_chain\n",
    "    }\n",
    "\n",
    "\n",
    "experiment = True\n",
    "\n",
    "for model in model_chains.keys():\n",
    "    chain = model_chains[model]\n",
    "    results = []\n",
    "    # # Experimentation : Iterate over each row\n",
    "    if experiment:\n",
    "        for i in  range(1,205,10):\n",
    "            dialogue_id = df.iloc[i]['dialogue_id']\n",
    "            conversation = df.iloc[i]['conversation']\n",
    "            intent = intent_dict[df.iloc[i]['service_call_method']]\n",
    "            if  intent != 'Reserve a Restaurant':continue\n",
    "\n",
    "            # Apply LLM chain on the desired column in each row\n",
    "            result = chain.invoke(\n",
    "                {\n",
    "                \"conversation\":dialogue_id +': '+ conversation,\n",
    "                \"intent\":intent\n",
    "                }\n",
    "            )\n",
    "            results.append(result.content)\n",
    "            time.sleep(5)\n",
    "\n",
    "    else:   \n",
    "\n",
    "        # # Iterate over each row\n",
    "        for index, row in df.iterrows():\n",
    "            # Apply LLM chain on the desired column in each row\n",
    "            result = chain.invoke({\n",
    "                'conversation' : row['dialogue_id']+': '+row['conversation'],\n",
    "                'intent' :intent_dict[row['service_call_method']]\n",
    "                })\n",
    "\n",
    "            results.append(result.content)\n",
    "            break\n",
    "            \n",
    "    print(f\"{model}: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (121049500.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[40], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    openai_chain.\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# df['instruction_openai']=results\n",
    "openai_chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out=df[['dialogue_id','service_call_method','conversation','instruction','service_call_parameters']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gemini AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gemini_chain = prompt | gemini_llm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Experimentation : Iterate over each row\n",
    "for i in  range(1,205,20):\n",
    "    dialogue_id = df.iloc[i]['dialogue_id']\n",
    "    conversation = df.iloc[i]['conversation']\n",
    "    intent = intent_dict[df.iloc[i]['service_call_method']]\n",
    "    if  intent != 'Reserve a Restaurant':continue\n",
    "\n",
    "    # Apply LLM chain on the desired column in each row\n",
    "    result = gemini_chain.invoke(  \n",
    "        {\n",
    "            \"intent\": intent,\n",
    "            \"conversation\":dialogue_id +': '+ conversation,\n",
    "        }\n",
    "    )\n",
    "    print(result.content)\n",
    "    time.sleep(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "7dcceaac5913125ee30d5c0a65a766a11a859ab54a1012faad86c3e5ea55483e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
